{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Consensus Scoring using Protein Conformational Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, sys\n",
    "sys.path.append('..')\n",
    "from modules.run_or_load_decorator import run_or_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Consensus scoring giving a set of selected conformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_name(*args, sep='/'):\n",
    "    return sep.join([*args])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_consensus_scoring(\n",
    "                              X, y,\n",
    "                              cs_method,\n",
    "                              test_size=0.25,\n",
    "                              metric_params=None\n",
    "                             ):\n",
    "    '''\n",
    "        Applies a consensus scoring method (cs_method) over a set (X) of dksc scores\n",
    "        and evaluates its performance using a evaluation metric (metric_params).\n",
    "        Returns a dataframe with the metric score value.\n",
    "    '''\n",
    "    consensus_score=cs_method(X)\n",
    "    \n",
    "    dict_y_preds={'y_pred': consensus_score}\n",
    "    \n",
    "    metric_results=PlotMetric(y_true=y,  y_pred_dict=dict_y_preds, \n",
    "           decreasing=False).format_metric_results(**metric_params)\n",
    "    \n",
    "    return(metric_results.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Xy_for_CS(\n",
    "                    X, y, \n",
    "                    selector, \n",
    "                    splitting_method, \n",
    "                    test_size=0.25,\n",
    "                    scaffold_series=None\n",
    "                    ):\n",
    "    '''\n",
    "        Splits X and y using a specific train-test splitting method\n",
    "    ''' \n",
    "\n",
    "    try: \n",
    "        scaffold_series\n",
    "    except NameError:\n",
    "        print(\"scaffold_series object doesn't exist\")\n",
    "\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # Initialize X_test and y_test as X and y\n",
    "    X_test, y_test = None, None\n",
    "    # If 'none' then do no perform a split and assume the whole set as test set\n",
    "    if splitting_method == 'none':\n",
    "        X_test, y_test = X, y\n",
    "    elif splitting_method == 'rand':\n",
    "        # Perform a random splitting\n",
    "        _, X_test, _, y_test = train_test_split(X, y, \n",
    "                                             test_size=test_size, stratify=y)\n",
    "    elif splitting_method == 'scff':\n",
    "        \n",
    "        _, X_test, _, y_test = train_test_scaffold_split(X, y, \n",
    "                                             scaffold_series=scaffold_series,\n",
    "                                             test_size=test_size, stratify=y)\n",
    "    return X_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CS_score_in_n_confs_range(X, y, splitting_method, selector,\n",
    "                                 cs_method, test_size, metric_params,\n",
    "                                 scaffold_series=None):\n",
    "    \n",
    "    # First verify that rfe_preselections object is in memory\n",
    "    try:\n",
    "       rfe_preselections \n",
    "    except NameError:\n",
    "        print(\"rfe_preselections object doesn't exist\")\n",
    "        \n",
    "    # Get the total number of selections\n",
    "    n_confs = X.shape[1]\n",
    "    # Initialize X_test and y_test \n",
    "    X_test, y_test = None, None\n",
    "    confs = None\n",
    "    # List of scoring results\n",
    "    results_list = []\n",
    "    \n",
    "    # Evaluate the Consensus method in range of 1 to n conformations\n",
    "    for k in range(1, n_confs + 1):\n",
    "        # *************\n",
    "        # Splitting\n",
    "        # *************\n",
    "        if splitting_method == 'rand':\n",
    "        # If is random, repeat splitting at each k step\n",
    "            X_test, y_test = split_Xy_for_CS(X, y, selector=selector, \n",
    "                                           splitting_method=splitting_method, test_size=test_size)\n",
    "        # If splitting method is not random, just do splittin once\n",
    "        elif k == 1:\n",
    "            X_test, y_test = split_Xy_for_CS(X, y, selector=selector, \n",
    "                                           splitting_method=splitting_method, test_size=test_size,\n",
    "                                           scaffold_series=scaffold_series)\n",
    "        \n",
    "        # ***********************\n",
    "        # Conformation Selection \n",
    "        # ***********************\n",
    "        if selector == 'rand':\n",
    "            # make a random sampling without replacement and select the first k positions\n",
    "            confs = np.random.choice(a=range(n_confs), size=n_confs, replace=False)[:k]\n",
    "        else:\n",
    "            confs = rfe_preselections[selector + '_' + splitting_method][:k] \n",
    "            \n",
    "        # Subset the X_test\n",
    "        X_test_confs = X_test.iloc[:, confs]\n",
    "        \n",
    "        # ****************\n",
    "        # Evaluation Score\n",
    "        # ****************\n",
    "        score = evaluate_consensus_scoring(\n",
    "                          X=X_test_confs, y=y_test, \n",
    "                          cs_method=cs_method, \n",
    "                          test_size=test_size,\n",
    "                          metric_params=metric_params)\n",
    "    \n",
    "        results_list.append(score.values[0][0])\n",
    "        \n",
    "    return np.array(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_CS_replicas(\n",
    "        nreps=15, \n",
    "        **kwargs\n",
    "    ):\n",
    "    \n",
    "    results_dict = {}\n",
    "    \n",
    "    for rep in range(nreps):\n",
    "        scores_list = get_CS_score_in_n_confs_range(**kwargs)\n",
    "        \n",
    "        results_dict[rep] = scores_list\n",
    "        \n",
    "    df = pd.DataFrame(results_dict)\n",
    "    # Get the mean and standard deviation\n",
    "    df = df.apply([np.mean, np.std], axis=1).fillna(0)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_or_load\n",
    "def aggregate_conf_selection_results_CS(\n",
    "        filename,\n",
    "        X,\n",
    "        y,\n",
    "        splitting_methods,\n",
    "        selectors,\n",
    "        cs_methods,\n",
    "        metrics,\n",
    "        test_size=0.25,\n",
    "        nreps=15,\n",
    "        scaffold_series=None\n",
    "    ):\n",
    "    \n",
    "    #*********************************************************\n",
    "    # Compute Results from Consensus Scoring over 1 to N confs\n",
    "    #*********************************************************\n",
    "    results_cs = []\n",
    "    for metric in metrics:\n",
    "        for split in splitting_methods:\n",
    "            for selector in selectors:\n",
    "                # Skip if RFE slector and none splitting\n",
    "                if split == 'none' and selector != 'rand':\n",
    "                     break\n",
    "                for cs_name, csm in cs_methods.items():\n",
    "                    metric_name = '_'.join([str(i) for i in metric.values()])\n",
    "                    print(format_name(split, selector, cs_name, metric_name)) \n",
    "                    df = do_CS_replicas(nreps=nreps,\n",
    "                                   X=X, y=y, \n",
    "                                   splitting_method=split,\n",
    "                                   selector=selector,\n",
    "                                   cs_method=csm, \n",
    "                                   test_size=test_size,\n",
    "                                   metric_params=metric,\n",
    "                                   scaffold_series=scaffold_series)\n",
    "                    # Format colnames\n",
    "                    df.columns = (format_name(split, selector, cs_name, metric_name, 'mean'),\n",
    "                                  format_name(split, selector, cs_name, metric_name, 'std'))\n",
    "\n",
    "                    results_cs.append(df.T)\n",
    "    X_cs = pd.concat(results_cs)\n",
    "    X_cs.index = X_cs.index.str.split('/', expand=True)\n",
    "    X_cs.rename_axis((\"split\", \"selector\", \"consensus\", \"metric\", \"desc\"), inplace=True)\n",
    "    \n",
    "    return X_cs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consensus Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Rank by number*\n",
    "<div style='background-color: #F9E5AB; min-height: 10px'></div>\n",
    "\n",
    "> The final score of each ligand $i$ ($R_bN_i$) corresponds to the *mean score* of ligand $i$ from a set of $n$ conformations.\n",
    "\n",
    "> $R_bN_i = \\frac{1}{n}\\sum_js_i^j,$\n",
    "\n",
    "where $n$ is the number of conformations, and $s_i^j$ is the docking score of the molecule $i$ with the $j$ conformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_by_number(df):\n",
    "    '''Get a dataframe of m*n values and returns a numpy array of means per row'''\n",
    "    # times -1 because roc_auc_score assumes higher is better\n",
    "    df_means = df.mean(axis = 1).to_numpy() * -1\n",
    "    return df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Best score (Merging and shrinking strategy)*\n",
    "<div style='background-color: #F9E5AB; min-height: 10px'></div>\n",
    "\n",
    "Also mentioned as *Merging and shrinking strategy* by Palacio-Rodríguez, et al. (2019).\n",
    "\n",
    "For each molecule $i$, get the lowest score of the $n$ scores calculated.\n",
    "> $\n",
    "\\begin{aligned}\n",
    "best\\ score_i = min(s_i),\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "where $s_i$ is a vector of $n$ scores, belonging to the molecule $i$, in which each position $j$ corresponds to the docking score between the molecule $i$ and each of the $n$ protein conformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_score(df):\n",
    "    '''Get a dataframe of m*n values and returns a numpy array of best scores per row'''\n",
    "    df_best = df.min(axis = 1)\n",
    "    # (times -1 because roc_auc_score in the following line assumes higher is better)\n",
    "    return df_best.to_numpy() * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Rank-by-rank*\n",
    "<div style='background-color: #F9E5AB; min-height: 10px'></div>\n",
    "\n",
    "For each conformation $j$ the $m$ molecules are ranked according to their docking score with that conformation. Therefore, the molecule with the best score is assigned with the value 1, the second one with the best score is assigned with the value 2, and so on, until the molecule with the worst value is assigned with the value $m$. Finally, the average rank of each molecule is compute taking into account its rank in all the $n$ conformations.\n",
    "\n",
    "> $\n",
    "\\begin{aligned}\n",
    "RbR_i = \\frac{1}{n}\\sum_jr^i_j,\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "where $n$ is the number of protein conformations, and $r^i_j$ is the rank position of the molecule $i$ in the ranked score obtained in the protein conformation $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_by_rank(df):\n",
    "    df_ranks = df.rank() # First, we get the ranking positions\n",
    "    # For each ligand i we get the average score among the n conformations\n",
    "    df_rank_by_rank = df_ranks.mean(axis = 1)\n",
    "    return df_rank_by_rank.to_numpy() * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Rank-by-vote*\n",
    "<div style='background-color: #F9E5AB; min-height: 10px'></div>\n",
    "Each molecule $i$ receives a vote if it is ranked on top $x\\%$ of the ranking list of each conformation. The final score of the molecule is given by  the sum of votes obtained in all the protein conformations.\n",
    "\n",
    "> $RbV_i(\\chi) = \\sum_j\\delta^i_j,$\n",
    "\n",
    "having:\n",
    "> - $\\delta _i = 1$ if $r^i_j \\leq \\chi m$\n",
    "- $\\delta _i = 0$ if $r^i_j > \\chi m$  \n",
    "\n",
    "where $\\chi$ is the top fraction of the docking scores ranking of $m$ molecules, $r^i$ is the rank of molecule $i$ in the ranking of the conformation $j$, and $n$ is the number of conformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_by_vote(df, chi = 10):\n",
    "    df_ranks = df.rank(ascending=True) # Get ranks, ascending=True; lower values get lower positions\n",
    "    x = np.floor(df.shape[0]/100 * chi) # Get the number of molecules inside chi\n",
    "    rbv_array = df_ranks.to_numpy() # Convert to a numpy array\n",
    "    # So, find values above chi (x) and assign the 0 or 1\n",
    "    rbv_array = np.where(rbv_array > x, 0, 1)\n",
    "    # Get the sum of votes\n",
    "    rb_votes = rbv_array.sum(axis = 1)\n",
    "    return rb_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Exponential Consensus Ranking*\n",
    "<div style='background-color: #F9E5AB; min-height: 10px'></div>\n",
    "\n",
    "It follows the same idea than the RIE and BEDROC metrics, taking into account the position of each molecule inside the ranked results following a exponential distribution. It is proposed by Palacio-Rodríguez, et al. (2019) as a scoring consensus using different docking programs:\n",
    "\n",
    "> \"*Exponential Consensus Ranking (ECR), a strategy to combine the results from\n",
    "several scoring functions/docking programs using an exponential distribution for each individual rank.*\"\n",
    "\n",
    "For each molecule $i$ at the protein conformation $j$ the *exponential score* $p(r^j_i)$ is calculated as follows:\n",
    "\n",
    "> $$p(r^j_i) = \\frac{1}{\\sigma}exp\\left( - \\frac{r_i^j}{\\sigma} \\right),$$\n",
    "\n",
    "where $p(r^j_i)$ is the rank of the molecule $i$ in the docking scoring results of the protein conformation $j$, and $\\sigma$ is the expected value of the exponential distribution. Just to remember, the exponential distribution (in the form $f(x) = \\lambda e^{(- \\lambda x)}$) has a mean and an standard deviation of $1/\\lambda$ which here in $ECR$ is referred as $\\sigma$: ($\\sigma = 1 / \\lambda$). As was said in a previous notebook for RIE and BEDROC metrics, the parameter $\\sigma$ can be understood as a fraction of the ranked list where the weight is important.\n",
    "As said by Palacio-Rodríguez, et al.:\n",
    "> \"*This parameter $(\\sigma)$ establishes the number of molecules for each scoring function that will be considered, i.e., the threshold of the dataset to be taken into account for the consensus. *\"\n",
    "\n",
    "Finally, the final $ECR$ score $P(i)$ of each molecule $i$ is computed as follows:\n",
    "\n",
    "> $$P(i) = \\sum_{j=1}^n p(r^j_i) = \\frac{1}{\\sigma} \\sum_{j=1}^n exp\\left( - \\frac{r_i^j}{\\sigma} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_consensus_ranking(df, sigma = 2):\n",
    "    # ascending=True; lower values get lower positions\n",
    "    ranks_array = df.rank(ascending=True).to_numpy()\n",
    "    # Apply the exponential function\n",
    "    exp_ranks = (1/ sigma)*(np.exp( - ranks_array/sigma))\n",
    "    \n",
    "    # For each ligand i we get the average score among the n conformations\n",
    "    ecs = exp_ranks.sum(axis = 1)\n",
    "    return ecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
