{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, estimator, estimator_hyparams = None,\n",
    "               scoring = 'roc_auc', k_folds = 5, standarize = True, \n",
    "                rfe = False, rfe_n_features = 2,\n",
    "               split_train = False, test_size = 0.2, random_state = 1, **kwargs):\n",
    "    '''If desire, the original train set cab be splitted. Just in case of been useful'''\n",
    "    if split_train:\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_train, y_train, test_size = test_size,\n",
    "                         stratify = y_train, random_state = random_state)\n",
    "    \n",
    "    '''\n",
    "     1) Pipeline is created, and will perform StandarScaler. More steps can be added later.\n",
    "     # pipe args is a list of tuples initialized with one element; the estimator.\n",
    "     # If standarize = true it adds StandardScaler at the bigining of the pipe.\n",
    "    '''\n",
    "    pipe_args = [(\"estimator\", estimator(**kwargs))]\n",
    "    \n",
    "    '''Recursive Feature Elimination'''\n",
    "    if rfe:\n",
    "        pipe_args.insert(0, (\"rfe\", RFE(estimator(**kwargs), \n",
    "                                        n_features_to_select = rfe_n_features,\n",
    "                                        step = 1)))\n",
    "    if standarize:\n",
    "        pipe_args.insert(0, (\"scaler\", StandardScaler()))\n",
    "    pipe = Pipeline(pipe_args)\n",
    "    '''\n",
    "    2) The specific hyperparameters of the selected stimator are given, \n",
    "    we will parse them to the gridSearch instance.\n",
    "    '''\n",
    "    if estimator_hyparams != None:\n",
    "        params = {}\n",
    "        for key, value in estimator_hyparams.items():\n",
    "            params['estimator__' + key] = value\n",
    "        '''\n",
    "        3) Grid search cross validation for turning the optimal parameters, \n",
    "        it takes the pipeline object. GridSearch performs k-fold cross validation, \n",
    "        and uses the given scoring method to validate each set.\n",
    "\n",
    "        '''\n",
    "        grid = GridSearchCV(estimator = pipe, param_grid = params, \n",
    "                            cv = k_folds, scoring = scoring,\n",
    "                            n_jobs = 6, refit=True)\n",
    "        estimator = grid\n",
    "        \n",
    "    else:\n",
    "        '''Additionaly, if estimator_hyparams is None, Grid search is avoided.'''\n",
    "        estimator = pipe\n",
    "        \n",
    "    '''SVC training through GridSearch object'''\n",
    "    estimator.fit(X_train, y_train)\n",
    "    '''Return the trained estimator (an instance from GridSearchCV or Pipeline)'''\n",
    "    #final_model = estimator.best_estimator_ if estimator_hyparams != None else estimator\n",
    "    return(estimator)\n",
    "\n",
    "def eval_model(model, X_test, y_test, return_proba = True):\n",
    "    '''\n",
    "    1) Predictions and evaluation on the Test set\n",
    "    - Scaling and prediction of X_train using the best model found by grid\n",
    "    '''\n",
    "    if return_proba:\n",
    "        y_prob  = model.predict_proba(X_test)[:,1] # Predicted prob values for X_test\n",
    "        y_hat = y_prob\n",
    "    else:\n",
    "        y_score = model.decision_function(X_test)\n",
    "        y_hat = y_score\n",
    "    y_pred  = model.predict(X_test) # predicted values\n",
    "    '''Returns the y_score values and the lnear_SVC object'''\n",
    "    return(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrapper(train_key, list_of_scores, estimator,\n",
    "                  scores_dic, selected_features = None, **kwargs):\n",
    "    ''''''\n",
    "    trained_models = {}\n",
    "    for score in list_of_scores:\n",
    "        if selected_features is None:\n",
    "            X_train = scores_dic[train_key][score]['X']\n",
    "        else:\n",
    "            X_train = scores_dic[train_key][score]['X'][selected_features]\n",
    "        y_train = scores_dic[train_key][score]['y']\n",
    "        name = F'{score}'\n",
    "        trained_models[name] = train_model(X_train, y_train, estimator, **kwargs)\n",
    "    return(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_wrapper(trained_model, test_keys, list_of_scores,\n",
    "                  scores_dic, selected_features = None, return_proba = True, **kwargs):\n",
    "    model_results = {}\n",
    "    for test_key in test_keys:\n",
    "        y_preds = {}\n",
    "        for score in list_of_scores:\n",
    "            if selected_features is None:\n",
    "                X_test  = scores_dic[test_key][score]['X']\n",
    "            else: \n",
    "                X_test  = scores_dic[test_key][score]['X'][selected_features]\n",
    "            y_test  = scores_dic[test_key][score]['y']\n",
    "            name = F'{test_key}-{score}'\n",
    "            y_preds[name] = eval_model(trained_model[score], X_test, y_test, return_proba = return_proba)\n",
    "        # Invoke PlotMetric Class\n",
    "        model_results[test_key] = PlotMetric(y_true = y_test, y_pred_dict = y_preds, **kwargs)\n",
    "    return(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predict_results(title, predictions, train_key, plot_rankings = True, plot_nef = False):\n",
    "    n_rows = 2 if plot_nef else 1\n",
    "    plt.figure(figsize=(14, 7*n_rows))\n",
    "    #plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    for i, test_set in enumerate(test_keys):\n",
    "        plt.subplot(F'{n_rows}2{i+1}')\n",
    "        predictions[test_set].plot_roc_auc(F'{title}:\\n{train_key} train, {test_set} test', \n",
    "                                     show_by_itself = False, fontsize = 'x-small')\n",
    "        if plot_nef:\n",
    "            plt.subplot(F'{n_rows}2{i+3}')\n",
    "            predictions[test_set].plot_ef_auc('', method = 'normalized', max_chi = 0.1, \n",
    "                                         show_by_itself = False, fontsize = 'x-small')\n",
    "    plt.show()\n",
    "    if plot_rankings:\n",
    "        for test_set in test_keys:\n",
    "            predictions[test_set].plot_actives_distribution(max_position_to_plot=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('mds': conda)",
   "language": "python",
   "name": "python37664bitmdsconda8fcfe9b6c3484566880235af88776fef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
