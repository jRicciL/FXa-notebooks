{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions: Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, sys, os\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.plotting_metrics import PlotMetric\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='white', context='talk', font_scale=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeout decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "from functools import wraps\n",
    "\n",
    "def timeout(n_seconds=300):\n",
    "    '''Stops a function execution after n seconds'''\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Set alarm for n seconds\n",
    "            signal.alarm(n_seconds)\n",
    "            try:\n",
    "                # Call decorated func\n",
    "                return func(*args, **kwargs)\n",
    "#             except TimeoutError as e:\n",
    "                print(f'Execution finished after {n_seconds}:', e)\n",
    "            finally:\n",
    "                # Cancel Alarm\n",
    "                signal.alarm(0)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(estimator, X, y, title, ylim=[0.5,1], axes=None,\n",
    "                         cv=3, train_sizes=np.linspace(0.1, 1.0, 10), \n",
    "                         scoring='roc_auc', n_jobs=4):\n",
    "    '''\n",
    "    Plot estimator performance on the training and validation\n",
    "    sets as a function of the training set size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: sklearn estimator object type\n",
    "       Object type that implements the \"fit\" and \"predict method\"\n",
    "\n",
    "    X: array-like, shape (m_samples, n_features)\n",
    "        Training array with m_samples and n_features.\n",
    "    y: array-like, shape (m_samples)\n",
    "        Target array relative to X with m labels.\n",
    "    axes: array of 3 axes\n",
    "        matplotlib axes array to append the generated plot\n",
    "    ylim: array\n",
    "       \n",
    "    '''\n",
    "    if axes == None:\n",
    "        _, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        \n",
    "    axes.set(title=title, ylim=ylim, xlabel='Training examples', ylabel=f'Metric: {scoring}')\n",
    "    # Use learning_curve function from sklearn\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "       learning_curve(estimator, X, y, scoring=scoring, return_times = True,\n",
    "                      cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    # Compute useful metrics\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "#     fit_times_mean = np.mean(fit_times, axis=1)\n",
    "#     fit_times_std = np.std(fit_times, axis=1)\n",
    "    # Plot the learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                        train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                        test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color='r',\n",
    "                label='Train score')\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color='g',\n",
    "                label='Cross-validation score')\n",
    "    axes.legend(loc='lower right')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tunning: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(estimator, X_train, y_train, X_test, y_test, hyperparams,  cv_value=5, \n",
    "                    scoring='roc_auc', splitting='random', std_scale=False,\n",
    "                   randomGS = False, n_iter=10):\n",
    "    # Format the hyperparms\n",
    "    hyperparams_dict = {'estimator__' + key: val for key, val in hyperparams.items()}\n",
    "    \n",
    "    if std_scale:\n",
    "        # Create the Pipe\n",
    "        scaler = StandardScaler()\n",
    "        pipe = Pipeline([('scaler', scaler),\n",
    "                         ('estimator', estimator)])\n",
    "    else:\n",
    "        pipe = estimator\n",
    "    \n",
    "    # Do Grid Search\n",
    "    if randomGS:\n",
    "        gs = RandomizedSearchCV(estimator = pipe,  param_distributions = hyperparams,\n",
    "                     cv = cv_value, scoring = scoring, n_jobs = 6, refit = True, n_iter=n_iter)\n",
    "    else:\n",
    "        gs = GridSearchCV(estimator = pipe, param_grid = hyperparams,\n",
    "                     cv = cv_value, scoring = scoring, n_jobs = 6, refit = True)\n",
    "    # Train the model\n",
    "    gs.fit(X_train, y_train) \n",
    "\n",
    "    # Predictions\n",
    "    y_train_predict = gs.predict_proba(X_train)\n",
    "    y_test_predict = gs.predict_proba(X_test)\n",
    "    \n",
    "    # Values to return\n",
    "    n_train_mols    = y_train.shape[0]\n",
    "    n_train_actives = y_train.sum()\n",
    "    n_test_mols     = y_test.shape[0]\n",
    "    n_test_actives  = y_test.sum()\n",
    "    mean_cv_roc     = gs.best_score_\n",
    "    train_roc       = roc_auc_score(y_train, y_train_predict[:, 1])\n",
    "    test_roc        = roc_auc_score(y_test, y_test_predict[:, 1])\n",
    "    best_params     = gs.best_params_\n",
    "\n",
    "    # Print some values\n",
    "    print(f'No. of molecules in train set: {n_train_mols}, with {n_train_actives} actives.')\n",
    "    print(f'No. of molecules in test set: {n_test_mols}, with {n_test_actives} actives.')\n",
    "    print('')\n",
    "    print('*'*10, 'GRID SEARCH RESULTS', '*'*10)\n",
    "    print('- Mean CV ROC-AUC:\\t{:.3f}'.format(mean_cv_roc))\n",
    "    print('- Train ROC-AUC:  \\t{:.3f}'.format(train_roc))\n",
    "    print('- Test ROC-AUC:   \\t{:.3f}'.format(test_roc))\n",
    "    print('- Best hyperparameters', best_params)\n",
    "    print('**'*21)\n",
    "    print('')\n",
    "    \n",
    "    return [n_train_mols, n_train_actives, n_test_mols, n_test_actives,\n",
    "            mean_cv_roc, train_roc, test_roc, best_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_cv(estimator, X_train, y_train,hyperparams,  cv_value=5, \n",
    "                    scoring='roc_auc', splitting='random', std_scale=True,\n",
    "                   randomGS = False, n_iter=10):\n",
    "    # Format the hyperparms\n",
    "    \n",
    "    if std_scale:\n",
    "        # Create the Pipe\n",
    "        hyperparams_dict = {'estimator__' + key: val for key, val in hyperparams.items()}\n",
    "        scaler = StandardScaler()\n",
    "        pipe = Pipeline([('scaler', scaler),\n",
    "                         ('estimator', estimator)])\n",
    "    else:\n",
    "        pipe = estimator\n",
    "    \n",
    "    # Do Grid Search\n",
    "    if randomGS:\n",
    "        gs = RandomizedSearchCV(estimator = pipe,  param_distributions = hyperparams_dict,\n",
    "                     cv = cv_value, scoring = scoring, n_jobs = 6, refit = True, n_iter=n_iter)\n",
    "    else:\n",
    "        gs = GridSearchCV(estimator = pipe, param_grid = hyperparams_dict,\n",
    "                     cv = cv_value, scoring = scoring, n_jobs = 6, refit = True)\n",
    "    # Train the model\n",
    "    gs.fit(X_train, y_train) \n",
    "    y_train_predict = gs.predict_proba(X_train)\n",
    "\n",
    "\n",
    "    # Values to return\n",
    "    n_train_mols    = y_train.shape[0]\n",
    "    n_train_actives = y_train.sum()\n",
    "\n",
    "    mean_cv_roc     = gs.best_score_\n",
    "    train_roc       = roc_auc_score(y_train, y_train_predict[:, 1])\n",
    "    \n",
    "    best_params     = gs.best_params_\n",
    "\n",
    "    # Print some values\n",
    "    print(f'No. of molecules in train set: {n_train_mols}, with {n_train_actives} actives.')\n",
    "    #print(f'No. of molecules in test set: {n_test_mols}, with {n_test_actives} actives.')\n",
    "    print('')\n",
    "    print('*'*10, 'GRID SEARCH RESULTS', '*'*10)\n",
    "    print('- Mean CV ROC-AUC:\\t{:.3f}'.format(mean_cv_roc))\n",
    "    print('- Train ROC-AUC:  \\t{:.3f}'.format(train_roc))\n",
    "    #print('- Test ROC-AUC:   \\t{:.3f}'.format(test_roc))\n",
    "    print('- Best hyperparameters', best_params)\n",
    "    print('**'*21)\n",
    "    print('')\n",
    "    \n",
    "    return gs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to report the Best Conformation's ROC-AUC for a given subset of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************************************\n",
    "# Returns the best conformatio's ROC-AUC value of a given subset X and y\n",
    "#************************************************************************\n",
    "\n",
    "def get_roc_auc_DkSc(X_train, y_train, X_test, y_test, verbose=True):\n",
    "    roc_auc_train = X_train.apply(\n",
    "        lambda x: roc_auc_score(y_true= y_train, y_score= -x), axis=0)\n",
    "    roc_auc_test = X_test.apply(\n",
    "        lambda x: roc_auc_score(y_true= y_test, y_score= -x), axis=0)\n",
    "    # Values to return\n",
    "    train_best_roc = roc_auc_train.max()\n",
    "    train_median   = roc_auc_train.median()\n",
    "    train_mean     = roc_auc_train.mean()\n",
    "    test_best_roc = roc_auc_test.max()\n",
    "    test_median   = roc_auc_test.median()\n",
    "    test_mean     = roc_auc_test.mean()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"***** Best Conformation's ROC-AUC using docking scores *****\")\n",
    "        \n",
    "        print(\"> Train best conf. ROC-AUC: {:.3f}\".format(train_best_roc) +\n",
    "              \" \\t> median: {:.3f}\".format(train_median) +\n",
    "              ', mean: {:.3f}'.format(train_mean))\n",
    "        \n",
    "        print(\"> Test best conf. ROC-AUC: {:.3f}\".format(test_best_roc) +\n",
    "              \" \\t> median: {:.3f}\".format(test_median) +\n",
    "              ', mean: {:.3f}'.format(test_mean))\n",
    "        print('**'*32)\n",
    "    # return a list of results to capture by the  wrapper function\n",
    "    return [train_best_roc, train_median, train_mean,\n",
    "            test_best_roc, test_median, test_mean]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************\n",
    "# Decorator functions to capture GS results\n",
    "#******************************************\n",
    "from functools import wraps\n",
    "\n",
    "def capture_GS_results(results_dict=None, capture=True):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if results_dict != None and capture:\n",
    "                results = func(*args, **kwargs)\n",
    "                # Create a key with the first four values\n",
    "                key = '_'.join(results[:4])\n",
    "                # Append results to the results dictionary\n",
    "                results_dict[key] = results\n",
    "            else:\n",
    "                return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "            \n",
    "#******************************************************\n",
    "# Dictionary to capture GS results using the decorator \n",
    "#******************************************************\n",
    "results_dict = {}\n",
    "\n",
    "#******************************************************\n",
    "# Function to Split and run Grid Search\n",
    "#******************************************************\n",
    "@capture_GS_results(results_dict)\n",
    "def split_and_gs(train_name, test_name, estimator_name,\n",
    "                 X, y, estimator, hyperparams, splitting='random', \n",
    "                 test_size=0.25, scaffold_series=None, random_state=None, **kwargs):\n",
    "    '''Given a X and y sets, a sklean estimator and an splitting method, \n",
    "    performs Grid Search CV using the parsed hyperparams'''\n",
    "    #**************\n",
    "    # Do the split\n",
    "    #**************\n",
    "    if splitting == 'scaffold':\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_scaffold_split(X, y, scaffold_series = scaffold_series,\n",
    "                test_size=test_size, stratify=y)\n",
    "    elif splitting == 'random':\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=test_size, stratify=y,\n",
    "                            random_state=random_state)\n",
    "        \n",
    "    # Verbose\n",
    "    print(f'{estimator_name} => Train: {train_name}; Test: {test_name}; split: {splitting}')\n",
    "    \n",
    "    # Function to run Grid Search\n",
    "    #----------------------------\n",
    "    gs_results = run_grid_search(estimator, \n",
    "                    X_train, y_train, X_test, y_test, \n",
    "                    hyperparams = hyperparams,  **kwargs)\n",
    "\n",
    "    # Function to extract ROC results from DkSc values \n",
    "    #-------------------------------------------------\n",
    "    dksc_results = get_roc_auc_DkSc(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Return both list of results to captured by the decorador function\n",
    "    return [train_name, test_name, estimator_name, splitting] + gs_results + dksc_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
