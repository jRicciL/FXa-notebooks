{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Evaluation: Conformational selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob, sys, os\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../modules/plotting_metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='white', context='talk', font_scale=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../6_Machine_Learning_Models/df_DkSc_results_COCRYS_DEKOIS_DUD.pkl'\n",
    "X_merged_dksc = pd.read_pickle(file_name)\n",
    "# Extract activity column\n",
    "y_true_merged = X_merged_dksc['activity']\n",
    "# Drop column from merged_dkksc\n",
    "X_merged_dksc = X_merged_dksc.drop('activity', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scaffold_splitter import train_test_scaffold_split\n",
    "\n",
    "# Compute or load the dataframe containing the Generic Murcko Scaffolds\n",
    "file = './df_COCRYS_DUD_DEKOIS_Murcko_Scaffolds_SMILES.obj'\n",
    "\n",
    "df_scff_murcko = pd.read_pickle(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary of results from notebook 6\n",
    "TRAIN_DB = 'MergedDB'\n",
    "TEST_DB = 'MergedDB'\n",
    "SCORE_TYPE = 'DkSc'\n",
    "scaffold_series = df_scff_murcko['scff_generic']\n",
    "\n",
    "FILEPATH = f'../6_Machine_Learning_Models/ml_models/conf_selection/{SCORE_TYPE}_{TRAIN_DB}-{TRAIN_DB}'\n",
    "\n",
    "def format_filename(*args):\n",
    "    return '_'.join([*args] ) + '.obj'\n",
    "\n",
    "def format_name(*args, sep='/'):\n",
    "    return sep.join([*args])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_best_dk_score(splitting_method, \n",
    "                           selector_name, \n",
    "                           classifier_name,\n",
    "                           X_dks,\n",
    "                           metric='roc_auc', **metric_params):\n",
    "    SPLIT = splitting_method\n",
    "    SELEC = selector_name\n",
    "    CLF = classifier_name\n",
    "    \n",
    "    # Read the file\n",
    "    if selector_name == 'rand':\n",
    "        sel_str =  f'randSel'\n",
    "    else:\n",
    "        sel_str =  f'rfeSel-{SELEC}'\n",
    "        \n",
    "    file = FILEPATH + f'_{SPLIT}Split_{CLF}_{sel_str}_nreps15.obj'\n",
    "    with open(file, 'rb') as f:\n",
    "        main_dict = pickle.load(f)\n",
    "        \n",
    "    ref_scores_list = []\n",
    "    # Iterate over reps\n",
    "    for rep in main_dict.keys():\n",
    "        test_index = main_dict[rep]['y_true_index']\n",
    "        y_true = main_dict[rep]['y_true']\n",
    "        \n",
    "        # Subset docking scores matrix (X_dksc) to evaluate using only those molecules\n",
    "        X_test_dks = X_dks.loc[test_index]\n",
    "        X_test_dks = {col: X_test_dks[col].values for col in X_test_dks.columns}\n",
    "        \n",
    "        # Compute the requested metric using the docking scores and keep the max score\n",
    "        metric_results = PlotMetric(y_true=y_true, y_pred_dict=X_test_dks, \n",
    "                                 decreasing=True).format_metric_results(metric_name=metric, **metric_params)\n",
    "        max_score = metric_results.max().values[0]\n",
    "        # Save the max score\n",
    "        ref_scores_list.append(max_score)\n",
    "        \n",
    "    # Get the max score and format as dataframe with a multiindex\n",
    "    # If the metric is EF or BEDROC and depends on a hyperparameter, concat this value to the metric name\n",
    "    if len({**metric_params}) > 0:\n",
    "        parm_str = str([*{**metric_params}.values()][0])\n",
    "        metric = metric + '_' + parm_str\n",
    "    \n",
    "    idx = pd.MultiIndex.from_product([[SPLIT], [SELEC], [CLF], [metric]])\n",
    "    max_score = max(ref_scores_list)\n",
    "    max_score_df = pd.DataFrame({'best_dksc': max_score}, index=idx)\n",
    "    return max_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>best_dksc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rand</th>\n",
       "      <th>RF</th>\n",
       "      <th>LogReg</th>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        best_dksc\n",
       "rand RF LogReg roc_auc      0.731"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_best_dk_score('rand', 'RF', 'LogReg', X_merged_dksc, 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conf_selection_results(splitting_method, \n",
    "                                   selector_name, \n",
    "                                   classifier_name,\n",
    "                                   metric='roc_auc',**metric_params):\n",
    "    '''Reads a pickle file which contains y_pred arrays for a given selection method for \n",
    "    a k number if conformations; with k is in range of 1 to N conformations.'''\n",
    "    SPLIT = splitting_method\n",
    "    SELEC = selector_name\n",
    "    CLF = classifier_name\n",
    "    \n",
    "    # Read the file\n",
    "    if selector_name == 'rand':\n",
    "        sel_str =  f'randSel'\n",
    "    else:\n",
    "        sel_str =  f'rfeSel-{SELEC}'\n",
    "        \n",
    "    file = FILEPATH + f'_{SPLIT}Split_{CLF}_{sel_str}_nreps15.obj'\n",
    "    with open(file, 'rb') as f:\n",
    "        main_dict = pickle.load(f)\n",
    "\n",
    "    dic_metrics = {}\n",
    "\n",
    "    # Iterate over reps\n",
    "    for rep in main_dict.keys():\n",
    "        # Extract y_true\n",
    "        y_true = main_dict[rep]['y_true']\n",
    "        # Extract the dict of predicted values\n",
    "        dict_y_preds = main_dict[rep].copy()\n",
    "        del dict_y_preds['y_true_index']\n",
    "\n",
    "        # Compute the evaluation metric\n",
    "        metric_results = PlotMetric(y_true=y_true, y_pred_dict=dict_y_preds, \n",
    "                                 decreasing=False).format_metric_results(metric_name=metric,\n",
    "                                                                         **metric_params)\n",
    "        dic_metrics[rep] = metric_results.T.values[0]\n",
    "\n",
    "    # convert results into a dataframe\n",
    "    df_metrics = pd.DataFrame(dic_metrics)\n",
    "    # Keep only mean and standard deviation\n",
    "    df_metrics = df_metrics.apply([np.mean, np.std], axis=1).fillna(0)\n",
    "    # Rename columns following the split-selection-classifier-metric pattern\n",
    "    df_metrics.columns = [format_name(SPLIT, SELEC, CLF, metric, 'mean'),\n",
    "                         format_name(SPLIT, SELEC, CLF, metric, 'std')]\n",
    "    # Return the transposed matrix\n",
    "    return df_metrics.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_methods = ['scff', 'rand']\n",
    "selectors = ['rand', 'LR', 'RF', 'XGB']\n",
    "classifiers = ['LogReg', 'rbfSVC', 'XGB_tree', '1NN']\n",
    "metrics = ['roc_auc', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-cb4103b7b6af>\u001b[0m in \u001b[0;36mprocess_best_dk_score\u001b[0;34m(splitting_method, selector_name, classifier_name, X_dks, metric, **metric_params)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Compute the requested metric using the docking scores and keep the max score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         metric_results = PlotMetric(y_true=y_true, y_pred_dict=X_test_dks, \n\u001b[0;32m---> 32\u001b[0;31m                                  decreasing=True).format_metric_results(metric_name=metric, **metric_params)\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mmax_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Save the max score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Doctorado/DOCTO_TESIS/Proteinas_Modelo/CDK2/FXa/ANALISIS/modules/plotting_metrics.py\u001b[0m in \u001b[0;36mformat_metric_results\u001b[0;34m(self, metric_name, only_keys, rounded, transposed, as_dataframe, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0mdic_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mas_dataframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Doctorado/DOCTO_TESIS/Proteinas_Modelo/CDK2/FXa/ANALISIS/modules/plotting_metrics.py\u001b[0m in \u001b[0;36m_get_roc_auc\u001b[0;34m(self, y_pred)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# ROC-AUC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_roc_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# pROC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mds/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mds/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mds/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mds/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 646\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mds/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m\"\"\"Like assert_all_finite, but only for ndarray.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# validation is also imported in extmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'assume_finite'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mds/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_dks = []\n",
    "\n",
    "# params = {'metric': 'ef', 'fraction': 0.02}\n",
    "params = {'metric': 'roc_auc'}\n",
    "\n",
    "for split in splitting_methods:\n",
    "    for selec in selectors:\n",
    "        for clf in classifiers:\n",
    "            result = process_best_dk_score(splitting_method = split, \n",
    "                                   selector_name = selec, \n",
    "                                   classifier_name = clf,\n",
    "                                   X_dks = X_merged_dksc,\n",
    "                                   **params)\n",
    "            results_dks.append(result)\n",
    "X_dks = pd.concat(results_dks)\n",
    "X_dks.rename_axis((\"split\", \"selector\", \"classifier\", \"metric\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = []\n",
    "\n",
    "# params = {'metric': 'ef', 'fraction': 0.02}\n",
    "params = {'metric': 'roc_auc'}\n",
    "\n",
    "for split in splitting_methods:\n",
    "    for selec in selectors:\n",
    "        for clf in classifiers:\n",
    "            result = process_conf_selection_results(splitting_method = split, \n",
    "                                   selector_name = selec, \n",
    "                                   classifier_name = clf,\n",
    "                                   **params)\n",
    "            results.append(result)\n",
    "\n",
    "X = pd.concat(results)\n",
    "X.index = X.index.str.split('/', expand=True)\n",
    "X.rename_axis((\"split\", \"selector\", \"classifier\", \"metric\", \"desc\"), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a plot\n",
    "query_X = \"split == 'scff' & selector == 'rand'\"\n",
    "\n",
    "# Results\n",
    "X_subset = X.query(query_X)\n",
    "X_subset = X_subset.reset_index().drop(['split', 'selector',  'metric', 0], axis=1)\n",
    "X_subset = X_subset.set_index(['desc', 'classifier']).T\n",
    "X_mean = X_subset.loc[:, 'mean']\n",
    "X_std = X_subset.loc[:, 'std']\n",
    "\n",
    "# Número de conformaciones\n",
    "n_confs = X_mean.shape[0]\n",
    "n_mols = X_mean.shape[1]\n",
    "print(n_mols)\n",
    "\n",
    "# Diccionario de colores\n",
    "cols_lines = {'LogReg': 'rgb(134, 102, 183)',\n",
    "              'rbfSVC': 'rgb(229, 156, 48)',\n",
    "              'XGB_tree': 'rgb(9, 153, 149)',\n",
    "              '1NN': 'rgb(230, 73, 79)'}\n",
    "cols_fill = {'LogReg': 'rgba(134, 102, 183, 0.25)',\n",
    "             'rbfSVC': 'rgba(216, 143, 48, 0.25)',\n",
    "             'XGB_tree': 'rgba(9, 153, 149, 0.25)',\n",
    "             '1NN': 'rgba(230, 73, 79, 0.25)'}\n",
    "\n",
    "# Dictionary names\n",
    "split_names = {'rand': 'Random', 'scff': 'Scaffold'}\n",
    "\n",
    "selector_names = {'rand': 'Random',\n",
    "                  'LR': 'RFE (Log. Reg.)',\n",
    "                  'RF': 'RFE (Rand. Forest)',\n",
    "                  'XGB': 'RFE (Grad. Boost)'}\n",
    "\n",
    "clf_names_dict = {'LogReg': 'Log. Regression',\n",
    "                  'rbfSVC': 'RBF SVM',\n",
    "                  'XGB_tree': 'Gradient Boosting',\n",
    "                  '1NN': '1-NN Classifier'}\n",
    "\n",
    "metric_names = {'roc_auc': 'ROC-AUC',\n",
    "                'nef_auc': 'Norm. EF',\n",
    "                'bedroc': 'BEDROC'}\n",
    "metric = 'roc_auc'\n",
    "\n",
    "traces = []\n",
    "for col in X_mean.columns:\n",
    "    # Create the upper and lower bounds\n",
    "    upper = X_mean[col] + X_std[col]\n",
    "    lower = X_mean[col] - X_std[col]\n",
    "    \n",
    "    upper = go.Scatter(x=X_mean.index, \n",
    "                       y=X_mean[col] + X_std[col],\n",
    "                       mode='lines',\n",
    "                       name=clf_names_dict[col], \n",
    "                       legendgroup=clf_names_dict[col], \n",
    "                       showlegend=False,\n",
    "                       line=dict(width=0),\n",
    "                       fillcolor=cols_fill[col],\n",
    "                       hoverinfo='skip',\n",
    "                       fill='tonexty')\n",
    "    \n",
    "    line = go.Scatter(x=X_mean.index, \n",
    "                       y=X_mean[col],\n",
    "                       mode='lines',\n",
    "                       name=clf_names_dict[col],\n",
    "                       hovertemplate = \n",
    "                       f'<b style=\"color: {cols_lines[col]}\">{clf_names_dict[col]}</b>' +\n",
    "                       '<br>' +\n",
    "                       '<b><i>k</i> confs:</b> %{x}' +\n",
    "                       '<br>' +\n",
    "                       f'<b><i>{metric_names[metric]}</i>:</b> ' + \n",
    "                       '%{y:.2f}' +\n",
    "                       '<extra></extra>',\n",
    "                       legendgroup=clf_names_dict[col], \n",
    "                       line=dict(width=2.5,\n",
    "                                 color=cols_lines[col]),\n",
    "                       fillcolor=cols_fill[col],\n",
    "                       fill='tonexty')\n",
    "    \n",
    "    lower = go.Scatter(x=X_mean.index, \n",
    "                       y=X_mean[col] - X_std[col],\n",
    "                       mode='lines',\n",
    "                       name=clf_names_dict[col], \n",
    "                       legendgroup=clf_names_dict[col], \n",
    "                       showlegend=False,\n",
    "                       hoverinfo='skip',\n",
    "                       line=dict(width=0),\n",
    "                      )\n",
    "    \n",
    "    traces = traces + [lower, line, upper]\n",
    "    \n",
    "fig = go.Figure(data=traces)   \n",
    "\n",
    "# Add ref DkSc best score\n",
    "# Ref score\n",
    "best_ref = X_dks.query(query_X).max()[0]\n",
    "fig.add_shape(dict(type='line', x0=0, x1=n_confs, y0=best_ref, y1=best_ref),\n",
    "             line=dict(color=\"#B7AF9E\", width=1.5, dash = 'dot'))\n",
    "\n",
    "fig.add_annotation(x=n_confs - 10, y=best_ref,\n",
    "                   showarrow=False,\n",
    "                   font=dict(\n",
    "                       size=9\n",
    "                   ),\n",
    "                   text='max Dksc: <b>{:.2f}</b>'.format(best_ref), \n",
    "                   bgcolor=\"#CEC9BD\")\n",
    "\n",
    "fig.update_xaxes(ticks='outside', showline=True, linewidth=2, linecolor='#43494F', mirror = True)\n",
    "fig.update_yaxes(range=[0.3, 1], tick0=0.00, dtick=0.05, )\n",
    "fig.update_yaxes(ticks='outside', showline=True, \n",
    "                 linewidth=2, linecolor='black', mirror = True)\n",
    "fig.update_layout(template='plotly_white',\n",
    "                  hoverlabel=dict(\n",
    "                     bgcolor = 'white',\n",
    "                     font_size=11.5\n",
    "                  ),\n",
    "                  xaxis = dict(\n",
    "                     title='Number of protein conformations used'\n",
    "                  ),\n",
    "                  yaxis = dict(\n",
    "                     title=f'Metric Score: <b>{metric_names[metric]}</b>'\n",
    "                  ),\n",
    "                  legend=dict(\n",
    "                     orientation=\"h\",\n",
    "                     yanchor=\"bottom\",\n",
    "                     y=0.02,\n",
    "                     xanchor=\"center\",\n",
    "                     x=0.5,\n",
    "                     bgcolor=\"#F5F3EF\"\n",
    "                    ))\n",
    "                  \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both dataframes as a pickle dictionary\n",
    "fxa_results = {'Scores'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
